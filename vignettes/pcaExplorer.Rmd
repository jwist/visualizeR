---
title: "PCA Explorer"
author: "Julien Wist"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{PCA Explorer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Understanding multivariate analysis
### a simple PCA example

Let's create a simple dataset. Because PCA is a very general mathematical method, it has been applied to many research area and thereby explained in many different terms. Let's choose a chemical representation of our dataset. Assume that we've prepared 8 solutions of known composition using 3 compounds. The "spectra" of each compound and the compositions are simply represented by one or zeros.

```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
noise <- matrix(runif(24) * 0.5, 3, 8)
Compositions <- t(array(c(1,0,0, 0,1,0, 0,0,1, 1,1,0, 1,0,1, 0,1,1, 1,1,1, 0,0,0), dim = c(3,8)))
labels <- c("100", "010", "001", "110", "101", "011", "111", "000")
```

The "spectra" of the three compounds look like this:
```{r, fig.show='hold'}
barplot(Elements[1,])
barplot(Elements[2,])
barplot(Elements[3,])
```

Each spectra has 4 variables.

If we assume that the compounds are not interacting and so on, the resulting spectra of each solution is a linear combination these spectra, according to the composition. We can thus create a matrix of data by multiplying both the composition and each element.

Les's look at the compositions:
```{r, echo=FALSE, results='asis'}
knitr::kable(Compositions)
```

and the resulting data matrix `M`:

```{r, fig.show='hold'}
M <- Compositions %*% Elements
knitr::kable(M)
```

This is important because PCA and other methods are factorization method that allows to factor a matrix into the product of two. Thus in this case, we hope that applying the PCA to our matrix of data, we should be able to recover the composition and the individual elements. Let's see if this works.

The first step is to scale the data. Then we can use one of the simplest method to compute a PCA, which is to find the eigenvalues. The most robust form of performing this task is by Singular Value Decomposition (SVD). This decomposition tries to find 3 matrices so that $$M = U \times D \times V^{\dagger}$$ 

```{r, fig.show='hold'}
SM <- scale(M)
SVD <- svd(SM)
# max(abs(SM - t(SVD$v %*% diag(SVD$d) %*% t(SVD$u)))) # difference 
```

After computing the SVD we can check how good the factorization performed by reconstructing the original matrix and taking the difference that is in this case `r max(abs(SM - t(SVD$v %*% diag(SVD$d) %*% t(SVD$u))))`.

The idea behind PCA is to reduce the dimension of the problem. Do we need 4 variables to describe correctly the data or is it possible to reduce this.

```{r, fig.show='hold'}
Maprox <- t(SVD$v[,1:2] %*% diag(SVD$d[1:2]) %*% t(SVD$u[,1:2]))
ERR <- max(abs(SM - Maprox))
```

Here we only use the two first principal components to reconstruct `M` and the error is still small: `r ERR`. This means that a representation of our data can be found that correctly describes the data with only two dimensions.

We can now look at this representation in two dimension that is called score plots. The scores are simple the rotation of the original scaled data, that is the multiplication of the data by the rotation vectors, or loadings.

```{r, fig.show='hold'}
scores <- SM %*% SVD$v[,1:2]
plot(scores[,1], scores[,2])
```

We said that the error was small, but we can do better at quantifying how good we perform. This is called the explained variance in the world of statistics and describes how much of our data are actually described using 1, 2, 3, or more principle components. This is related to the eigenvalues and is readily computed.

```{r, fig.show='hold'}
EXVAR <- sapply(SVD$d,function(x){100*x/sum(SVD$d)})
CUMVAR <- cumsum(EXVAR)
barplot(CUMVAR)
```

with two principle components we can reproduce almost 80% of our original data, `M`. While using 3 we can explain 100%. This should not surprise us, since all our solutions were indeed prepared using 3 compounds. So mixing 3 components should allow to describe a 3-compounds mixture, or linear combination.

If our factorization is correct, we should recover both the composition and the elements. Let's see the composition

```{r, fig.show='hold'}
for (i in 1:8) {
 barplot(SVD$u[i,1:3]) 
}
```

Clearly, the first three columns of `U` are our compositions. The matrix `V` contains the elements:
 
```{r, fig.show='hold'}
for (i in 1:4) {
 barplot(SVD$v[,i]) 
}
```
At least the first three, since we have only three different compounds. It is thus possible to factorize the original data matrix `M` and recover the original composition and the individual elements. It has to be noted that only 4 points are visible on the score plot, while 8 points (8 compositions) are expected. This is because some compositions are similar.

```{r, fig.show='hold'}
cbind(labels, scores)
```
## Classification

We can apply the same idea to classify solution that are similar.

```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
Compositions <- t(array(c(1,2,0.1, 1,2.05,0.1, 1,2.1,0.1, 2,1,0.82, 1.9,1,0.9, 1.99,1,0.8, 1.06,2,0.05, 1,2,0.12, 2,1,1, 2,0.9,0.8), dim = c(3,10)))
M <- Compositions %*% Elements
SM <- scale(M)
SVD <- svd(SM)
scores <- SM %*% SVD$v[,1:2]
plot(scores[,1], scores[,2])
```

Looking at the screeplot, the two first principal components explains almost 95% of the data. 
```{r, fig.show='hold'}
EXVAR <- sapply(SVD$d,function(x){100*x/sum(SVD$d)})
CUMVAR <- cumsum(EXVAR)
barplot(CUMVAR)
```

The matrix `V` contains the elements, but in this case the interpretation we make is different. Looking at our composition it is clear that the first and second compounds are forming two classes, the class with almost the double of compound 1 and the class with more of compound 2. Looking at the loadings, we see that the second compound is discriminant (the first loading) while the second loading shows a difference for compound 1. 
 
```{r, fig.show='hold'}
for (i in 1:4) {
 barplot(SVD$v[,i]) 
}
```

As a conclusion, the two first principal components suffice to classify the data into two categories. In this latter case, the loadings are the individual components that are most important to the classification, i.e., the most important variables.

## Alternative methods and built-in functions

Althouth it goes behond the scope of this topic to explain how to compute PCA from a numerical point of view, it is important to understand that it require the decomposition of the matrix into eigenvalues and eigenvectors. There are two main techniques to achieve this, the most reliable is using SVD, while the same results may be often obtained using a correlation matrix. Here a few lines of code to show that result is the same for our simple example.

The data:
```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
Compositions <- t(array(c(1,0,0, 0,1,0, 0,0,1, 1,1,0, 1,0,1, 0,1,1, 1,1,1, 0,0,0), dim = c(3,8)))
M <- Compositions %*% Elements
```

and the scores:
```{r, fig.show='hold'}
C <- cor(M)
S <- eigen(C)
scores <- M %*% S$vectors[,1:2]
plot(scores[,1], scores[,2])
```

While it is important to understand the underlying concepts of PCA, most software for statistics have built-in functions. Here is an example of such, that relies on SVD.

```{r, fig.show='hold'}
pca <- prcomp(M, scale = TRUE)
pca
pc <- c(1,2)
ev <- c(round(pca$sdev[pc[1]]/sum(pca$sdev)*100,0),
        round(pca$sdev[pc[2]]/sum(pca$sdev)*100,0),
        round(pca$sdev[pc[3]]/sum(pca$sdev)*100,0))
plot(pca$x[,pc[1]], pca$x[,pc[2]], col=labels, cex=0.7,
     xlab=paste0("PC ", pc[1], " (", ev[pc[1]], "%)"),
     ylab=paste0("PC ", pc[2], " (", ev[pc[2]], "%)"))
```


> "There are three kind of lies: lies, damn lies and statistics"

---
title: "PCA Explorer"
author: "Julien Wist"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{PCA Explorer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Understanding multivariate analysis
### a simple PCA example

Let's create a simple dataset. Because PCA is a very general mathematical method, it has been applied to many research area and thereby explained in many different terms. Let's choose a chemical representation of our dataset. Assume that we've prepared 8 solutions of known composition using 3 compounds. The "spectra" of each compound and the compositions are simply represented by one or zeros.

```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
noise <- matrix(runif(24) * 0.5, 3, 8)
Compositions <- t(array(c(1,0,0, 0,1,0, 0,0,1, 1,1,0, 1,0,1, 0,1,1, 1,1,1, 0,0,0), dim = c(3,8)))
labels <- c("100", "010", "001", "110", "101", "011", "111", "000")
```

The "spectra" of the three compounds look like this:
```{r, fig.show='hold'}
barplot(Elements[1,])
barplot(Elements[2,])
barplot(Elements[3,])
```

Each spectra has 4 variables.

If we assume that the compounds are not interacting and so on, the resulting spectra of each solution is a linear combination these spectra, according to the composition. We can thus create a matrix of data by multiplying both the composition and each element.

```{r, fig.show='hold'}
M <- Compositions %*% Elements
```

This is important because PCA and other methods are factorization method that allows to factor a matrix into the product of two. Thus in this case, we hope that applying the PCA to our matrix of data, we should be able to recover the composition and the individual elements. Let's see if this works.

The first step is to scale the data. Then we can use one of the simplest method to compute a PCA, which is to find the eigenvalues. The most robust form of performing this task is by Singular Value Decomposition (SVD). This decomposition tries to find 3 matrices so that $$M = U \times D \times V^{\dagger}$$ 

```{r, fig.show='hold'}
SM <- scale(M)
SVD <- svd(SM)
# max(abs(SM - t(SVD$v %*% diag(SVD$d) %*% t(SVD$u)))) # difference 
```

After computing the SVD we can check how good the factorization performed by reconstructing the original matrix and taking the difference that is in this case `r max(abs(SM - t(SVD$v %*% diag(SVD$d) %*% t(SVD$u))))`.

The idea behind PCA is to reduce the dimension of the problem. Do we need 4 variables to describe correctly the data or is it possible to reduce this.

```{r, fig.show='hold'}
Maprox <- t(SVD$v[,1:2] %*% diag(SVD$d[1:2]) %*% t(SVD$u[,1:2]))
ERR <- max(abs(SM - Maprox))
```

Here we only use the two first principal components to reconstruct `M` and the error is still small: `r ERR`. This means that a representation of our data can be found that correctly describes the data with only two dimensions.

We can now look at this representation in two dimension that is called score plots. The scores are simple the rotation of the original scaled data, that is the multiplication of the data by the rotation vectors, or loadings.

```{r, fig.show='hold'}
scores <- SM %*% SVD$v[,1:2]
plot(scores[,1], scores[,2])
```

We said that the error was small, but we can do better at quantifying how good we perform. This is called the explained variance in the world of statistics and describes how much of our data are actually described using 1, 2, 3, or more principle components. This is related to the eigenvalues and is readily computed.

```{r, fig.show='hold'}
EXVAR <- sapply(SVD$d,function(x){100*x/sum(SVD$d)})
CUMVAR <- cumsum(EXVAR)
barplot(CUMVAR)
```

with two principle components we can reproduce almost 80% of our original data, `M`. While using 3 we can explain 100%. This should not surprise us, since all our solutions were indeed prepared using 3 compounds. So mixing 3 components should allow to describe a 3-compounds mixture, or linear combination.

If our factorization is correct, we should recover both the composition and the elements. Let's see the composition

```{r, fig.show='hold'}
for (i in 1:8) {
 barplot(SVD$u[i,1:3]) 
}
```

Clearly, the first three columns of `U` are our compositions. The matrix `V` contains the elements:
 
```{r, fig.show='hold'}
for (i in 1:4) {
 barplot(SVD$v[,i]) 
}
```
At least the first three, since we have only three different compounds. It is thus possible to factorize the original data matrix `M` and recover the original composition and the individual elements. It has to be noted that only 4 points are visible on the score plot, while 8 points (8 compositions) are expected. This is because some compositions are similar.

```{r, fig.show='hold'}
cbind(labels, scores)
```
## Classification

We can apply the same idea to classify solution that are similar.

```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
Compositions <- t(array(c(1,2,0.1, 1,2.05,0.1, 1,2.1,0.1, 2,1,0.82, 1.9,1,0.9, 1.99,1,0.8, 1.06,2,0.05, 1,2,0.12, 2,1,1, 2,0.9,0.8), dim = c(3,10)))
M <- Compositions %*% Elements
SM <- scale(M)
SVD <- svd(SM)
scores <- SM %*% SVD$v[,1:2]
plot(scores[,1], scores[,2])
```

Looking at the screeplot, the two first principal components explains almost 95% of the data. 
```{r, fig.show='hold'}
EXVAR <- sapply(SVD$d,function(x){100*x/sum(SVD$d)})
CUMVAR <- cumsum(EXVAR)
barplot(CUMVAR)
```

The matrix `V` contains the elements, but in this case the interpretation we make is different. Looking at our composition it is clear that the first and second compounds are forming two classes, the class with almost the double of compound 1 and the class with more of compound 2. Looking at the loadings, we see that the second compound is discriminant (the first loading) while the second loading shows a difference for compound 1. 
 
```{r, fig.show='hold'}
for (i in 1:4) {
 barplot(SVD$v[,i]) 
}
```

As a conclusion, the two first principal components suffice to classify the data into two categories. In this latter case, the loadings are the individual components that are most important to the classification, i.e., the most important variables.

## Alternative methods and built-in functions

```{r, fig.show='hold'}
Elements <- t(array(c(1,0,0,0,0,1,0,0,0,0,1,1), dim = c(4,3)))
Compositions <- t(array(c(1,0,0, 0,1,0, 0,0,1, 1,1,0, 1,0,1, 0,1,1, 1,1,1, 0,0,0), dim = c(3,8)))
M <- Compositions %*% Elements
```

```{r, fig.show='hold'}
C <- cor(M)
S <- eigen(C)
scores <- M %*% S$vectors[,1:2]
plot(scores[,1], scores[,2])
```

```{r, fig.show='hold'}
pca <- prcomp(M, scale = TRUE)
pca
score(pca)
```


## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
